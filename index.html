<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shijie Ma</title>

    <meta name="author" content="Shijie Ma">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="basic/favicon.ico" type="image/x-icon"> -->
    <link rel="icon" href="basic/bear_dada.jpg" type="image/jpg">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <!-- Basic Information -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shijie Ma
                </p>
                <p>I'm currently a fourth-year Ph.D. student at <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>, supervised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>.
                </p>
                <p>
                  Before that, I obtained my B.Eng. degree from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2021, where I was supervised by Prof. <a href="https://scholar.google.com/citations?user=qCfE--MAAAAJ&hl=en">Guoqi Li</a> and Prof. <a href="https://faculty.dpi.tsinghua.edu.cn/lpshi.html">Luping Shi</a>.
                </p>
                <p>
                  My previous research interests lie in open-world machine learning, encompassing novel class discovery, continual learning and data-centric learning.
                  Currently, I focus on multimodal understanding and generation, particularly the relationship and synergy between them.
                </p>
                <p style="text-align:center">
                  <a href="mailto:mashijie2021@ia.ac.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=pLVzF3cAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mashijie1028/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="basic/mashijie.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="basic/mashijie.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <!--/ Basic Information -->

          <!-- Education -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Education</h2>
                <!-- ul style="list-style-type: disc; padding: 0;"-->
                <ul style="list-style-type: disc; padding-left: 20px;">
                  <li style="margin: 5px;">
                    <b>Ph.D.</b> at <b><a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a></b>, 2021-2026 (expected)
                    <br><span>Advisor: Prof. <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a></span>
                  </li>
                  <li style="margin: 5px;">
                    <b>B.E.</b> at <b><a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a></b>, 2017-2021
                    <br><span>Advisor: Prof. <a href="https://scholar.google.com/citations?user=qCfE--MAAAAJ&hl=en">Guoqi Li</a> and Prof. <a href="https://faculty.dpi.tsinghua.edu.cn/lpshi.html">Luping Shi</a></span>
                  </li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          <!--/ Education -->


          <!-- Preprints -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Preprints</h2>
              <p>
                * indicates equal contribution
              </p>
            </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!--/ Preprints -->


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/GenHancer.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://geyuying.github.io/">Yuying Ge</a>, <a href="http://ttengwang.com/">Teng Wang</a>, <a href="https://scholar.google.com/citations?user=x_0spxgAAAAJ&hl=en">Yuxin Guo</a>, <a href="https://geyixiao.com/">Yixiao Ge</a>, <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>
              <br>
              <a href="https://arxiv.org/abs/2503.19480">arXiv</a> / <a href="https://mashijie1028.github.io/GenHancer/">Project Page</a> / <a href="https://arxiv.org/abs/2503.19480">Code</a> / <a href="https://huggingface.co/msj9817/GenHancer">Model</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/OWL.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Open-world Machine Learning: A Review and New Outlooks</span>
              <br>
              <a href="https://impression2805.github.io/">Fei Zhu*</a>, <strong>Shijie Ma*</strong>, <a href="https://scholar.google.com/citations?user=zcwWjhUAAAAJ&hl=en">Zhen Cheng</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>, <a href="https://scholar.google.com/citations?user=qxWfV6cAAAAJ&hl=en">Zhaoxiang Zhang</a>, <a href="https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=en">Cheng-Lin Liu</a>
              <br>
              <a href="https://arxiv.org/abs/2403.01759">arXiv</a>
              <br>
            </td>
          </tr>


          <!-- Publications -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Publications</h2>
            </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!--/ Publications -->


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICLR25_Dolphin.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Aligned Better, Listen Better For Audio-Visual Large Language Models</span>
              <br>
              <a href="https://scholar.google.com/citations?user=x_0spxgAAAAJ&hl=en">Yuxin Guo</a>, <a href="https://scholar.google.com/citations?user=dNhzCu4AAAAJ&hl=en">Shuailei Ma</a>, <strong>Shijie Ma</strong>, <a href="https://scholar.google.com/citations?user=gSI_eiIAAAAJ&hl=en">Xiaoyi Bao</a>, <a href="https://scholar.google.com/citations?user=UHCDCRMAAAAJ&hl=en">Chen-Wei Xie</a>, <a href="https://zkcys001.github.io/">Kecheng Zheng</a>, Tingyu Weng, <a href="https://scholar.google.com/citations?user=IK9DwSIAAAAJ&hl=en">Siyang Sun</a>, <a href="https://scholar.google.com/citations?user=-hFpScAAAAAJ&hl=en">Yun Zheng</a>, <a href="https://people.ucas.ac.cn/~zouwei">Wei Zou</a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025
              <br>
              <a href="https://openreview.net/forum?id=1SYUKPeM12">Paper</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/PR25_TrustDD.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Towards Trustworthy Dataset Distillation</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <a href="https://scholar.google.com/citations?user=zcwWjhUAAAAJ&hl=en">Zhen Cheng</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>
              <br>
              <em>Pattern Recognition (<strong>PR</strong>)</em>, 2025
              <br>
              <a href="https://doi.org/10.1016/j.patcog.2024.110875">Paper</a> / <a href="https://arxiv.org/abs/2307.09165">arXiv</a> / <a href="https://github.com/mashijie1028/TrustDD">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/NeurIPS24_MSPE.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution</span>
              <br>
              <a href="https://scholar.google.com/citations?user=toVhUOgAAAAJ&hl=en">Wenzhuo Liu</a>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <strong>Shijie Ma</strong>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>
              <br>
              <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/3396657fe1a3c9a43ac7cd809c51a41e-Abstract-Conference.html">Paper</a> / <a href="https://arxiv.org/abs/2405.18240">arXiv</a> / <a href="https://github.com/SmallPigPeppa/MSPE">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/NeurIPS24_Happy.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Happy: A Debiased Learning Framework for Continual Generalized Category Discovery</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <a href="https://zhunzhong.site/">Zhun Zhong</a>, <a href="https://scholar.google.com/citations?user=toVhUOgAAAAJ&hl=en">Wenzhuo Liu</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>
              <br>
              <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/5ae0f7cfd65d8e2b39da4177fef82015-Abstract-Conference.html">Paper</a> / <a href="https://arxiv.org/abs/2410.06535">arXiv</a> / <a href="https://github.com/mashijie1028/Happy-CGCD">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ECCV24_WPSSAM.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation Models</span>
              <br>
              Xin-Jian Wu, <a href="https://evergrow.github.io/">Ruisong Zhang</a>, Jie Qin, <strong>Shijie Ma</strong>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024 (<em><strong style="color:red;">Oral</strong></em>)
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-72784-9_18">Paper</a> / <a href="https://arxiv.org/abs/2407.10131">arXiv</a> / <a href="https://github.com/xjwu1024/WPS-SAM">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CVPR24_CrossMAE.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training</span>
              <br>
              <a href="https://scholar.google.com/citations?user=x_0spxgAAAAJ&hl=en">Yuxin Guo</a>, <a href="https://scholar.google.com/citations?user=IK9DwSIAAAAJ&hl=en">Siyang Sun</a>, <a href="https://scholar.google.com/citations?user=dNhzCu4AAAAJ&hl=en">Shuailei Ma</a>, <a href="https://zkcys001.github.io/">Kecheng Zheng</a>, <a href="https://scholar.google.com/citations?user=gSI_eiIAAAAJ&hl=en">Xiaoyi Bao</a>, <strong>Shijie Ma</strong>, <a href="https://people.ucas.ac.cn/~zouwei">Wei Zou</a>, <a href="https://scholar.google.com/citations?user=-hFpScAAAAAJ&hl=en">Yun Zheng</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_CrossMAE_Cross-Modality_Masked_Autoencoders_for_Region-Aware_Audio-Visual_Pre-Training_CVPR_2024_paper.html">Paper</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CVPR24_AGCD.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Active Generalized Category Discovery</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <a href="https://zhunzhong.site/">Zhun Zhong</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_Active_Generalized_Category_Discovery_CVPR_2024_paper.html">Paper</a> / <a href="https://arxiv.org/abs/2403.04272">arXiv</a> / <a href="https://github.com/mashijie1028/ActiveGCD">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/XPL.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization</span>
              <br>
              <a href="https://scholar.google.com/citations?user=x_0spxgAAAAJ&hl=en">Yuxin Guo</a>, <strong>Shijie Ma</strong>, Yuhao Zhao, <a href="https://people.ucas.ac.cn/~zouwei">Wei Zou</a>
              <br>
              <em>International Conference on Acoustics, Speech, and Signal Processing (<strong>ICASSP</strong>)</em>, 2024
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10447949">Paper</a> / <a href="https://arxiv.org/abs/2403.03095">arXiv</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/NeurIPS23_DMT.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization</span>
              <br>
              <a href="https://scholar.google.com/citations?user=x_0spxgAAAAJ&hl=en">Yuxin Guo</a>, <strong>Shijie Ma</strong>, <a href="https://people.ucas.edu.cn/~suhu">Hu Su</a>, Zhiqing Wang, Yuhao Zhao, <a href="https://people.ucas.ac.cn/~zouwei">Wei Zou</a>, <a href="https://scholar.google.com/citations?user=IK9DwSIAAAAJ&hl=en">Siyang Sun</a>, <a href="https://scholar.google.com/citations?user=-hFpScAAAAAJ&hl=en">Yun Zheng</a>
              <br>
              <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/98143953a7fd1319175b491888fc8df5-Abstract-Conference.html">Paper</a> / <a href="https://arxiv.org/abs/2403.03145">arXiv</a> / <a href="https://github.com/gyx-gloria/DMT">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CVPR24_AGCD.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Rethinking Pretraining as a Bridge From ANNs to SNNs</span>
              <br>
              <a href="https://lyh983012.github.io/">Yihan Lin</a>, <a href="https://scholar.google.com/citations?user=NBsnGgoAAAAJ&hl=en">Yifan Hu</a>, <strong>Shijie Ma</strong>, <a href="https://manutdmoon.github.io/">Dongjie Yu</a>, <a href="https://scholar.google.com/citations?user=qCfE--MAAAAJ&hl=en">Guoqi Li</a>
              <br>
              <em>IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>)</em>, 2022
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9950361">Paper</a> / <a href="https://arxiv.org/abs/2203.01158">arXiv</a> / <a href="https://github.com/lyh983012/SNN-ANN-Pretrain">Code</a>
              <br>
            </td>
          </tr>





          <!-- Honors and Awards -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Honors and Awards</h2>
                <p>
                  <li style="margin: 5px;"> 
                    National Scholarship, 2024
                  </li>
                  <li style="margin: 5px;"> 
                    NeurIPS Top Reviewer, 2024
                  </li>
                  <li style="margin: 5px;"> 
                    Merit Student, Chinese Academy of Sciences, 2022
                  </li>
                  <li style="margin: 5px;"> 
                    Freshmen Scholarship, Chinese Academy of Sciences, 2021
                  </li>
                  <li style="margin: 5px;"> 
                    Comprehensive Scholarship, Tsinghua University, 2020
                  </li>
                  <li style="margin: 5px;"> 
                    Academic Scholarship, Tsinghua University, 2019
                  </li>
                </p>
              </td>
            </tr>
          </tbody></table>
          <!--/ Honors and Awards -->

          <!-- Academic Services -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Academic Services</h2>
                <p>
                  <li style="margin: 5px;"> 
                    <b>Conference Reviewer:</b>  NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, AISTATS, WACV, ICPR
                  </li>
                  <li style="margin: 5px;"> 
                    <b>Journal Reviewer:</b> IJCV, IEEE TKDE, IEEE TCSVT
                  </li>
                </p>
              </td>
            </tr>
          </tbody></table>
          <!--/ Academic Services -->

          <!-- template -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <a href="https://github.com/jonbarron/jonbarron_website">Website Template</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          <!--/ template -->

        </td>
      </tr>
    </table>
  </body>
</html>
