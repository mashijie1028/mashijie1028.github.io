<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shijie Ma</title>

    <meta name="author" content="Shijie Ma">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="basic/favicon.ico" type="image/x-icon"> -->
    <link rel="icon" href="basic/bear_dada.jpg" type="image/jpg">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <!-- Basic Information -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shijie Ma
                </p>
                <p>I'm currently a final Ph.D. student at <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>, supervised by Prof. <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>.
                </p>
                <p>
                  Before that, I obtained my B.Eng. degree from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2021, where I was supervised by Prof. <a href="https://scholar.google.com/citations?user=qCfE--MAAAAJ&hl=en">Guoqi Li</a> and Prof. <a href="https://faculty.dpi.tsinghua.edu.cn/lpshi.html">Luping Shi</a>.
                </p>
                <p>
                  Currently, I focus on multimodal understanding, reinforcement learning, agents, as well as long video understanding.
                </p>
                <p style="text-align:center">
                  <a href="mailto:mashijie2021@ia.ac.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=pLVzF3cAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mashijie1028/">Github</a> &nbsp;/&nbsp;
                  <a href="https://dblp.org/pid/191/4553.html">DBLP</a> &nbsp;/&nbsp;
                  <a href="https://x.com/msj_1028">Twitter</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="basic/mashijie.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="basic/mashijie.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <!--/ Basic Information -->

          <!-- Education -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Education</h2>
                <!-- ul style="list-style-type: disc; padding: 0;"-->
                <ul style="list-style-type: disc; padding-left: 20px;">
                  <li style="margin: 5px;">
                    <b>Ph.D.</b> @ <b><a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a></b>, 2021-2026 (expected)
                    <br><span>Advisor: Prof. <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a></span>
                  </li>
                  <li style="margin: 5px;">
                    <b>B.E.</b> @ <b><a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a></b>, 2017-2021
                    <br><span>Advisor: Prof. <a href="https://scholar.google.com/citations?user=qCfE--MAAAAJ&hl=en">Guoqi Li</a> and Prof. <a href="https://faculty.dpi.tsinghua.edu.cn/lpshi.html">Luping Shi</a></span>
                  </li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          <!--/ Education -->


          <!-- News -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <!-- ul style="list-style-type: disc; padding: 0;"-->
                <ul style="list-style-type: disc; padding-left: 20px;">
                  <li style="margin: 5px;">
                    <b>[2026.02]</b> AudioStory is accepted to <a href="https://cvpr.thecvf.com/Conferences/2026">CVPR 2026</a>.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2026.01]</b> Excited to release <a href="https://liuwq-bit.github.io/VideoTemp-o3/">VideoTemp-o3</a>, an agentic thinking-with-videos framework for long video understanding and temporal grounding.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2026.01]</b> One paper is accepted to <a href="https://iclr.cc/Conferences/2026">ICLR 2026</a>.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2025.08]</b> Excited to release <a href="https://github.com/TencentARC/AudioStory">AudioStory</a>, the first unified audio-language multimodal model for long-form audio generation, as well as audio understanding.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2025.06]</b> My paper GenHancer is accepted to <a href="https://iccv.thecvf.com/Conferences/2025">ICCV 2025</a>.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2025.05]</b> I have been selected as a Top Reviewer in <a href="https://icml.cc/Conferences/2025">ICML 2025</a>.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2025.03]</b> Excited to release <a href="https://mashijie1028.github.io/GenHancer/">GenHancer</a>, in which we systemically explore how generative models enhance multimodal understanding and provide several key insights.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2025.03]</b> My paper is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>!!!
                  </li>
                  <li style="margin: 5px;">
                    <b>[2025.01]</b> One paper is accepted to <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a>.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2024.12]</b> I have been selected as a Top Reviewer in <a href="https://neurips.cc/Conferences/2024">NeurIPS 2024</a>.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2024.09]</b> Two papers are accepted to <a href="https://neurips.cc/Conferences/2024">NeurIPS 2024</a>.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2024.07]</b> One paper is accepted to <a href="https://eccv.ecva.net/Conferences/2024">ECCV 2024</a> as an oral presentation.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2024.02]</b> Two papers are accepted to <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a>.
                  </li>
                  <li style="margin: 5px;">
                    <b>[2023.09]</b> One papers is accepted to <a href="https://neurips.cc/Conferences/2023">NeurIPS 2023</a>.
                  </li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          <!--/ News -->


          <!-- Preprints -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Preprints</h2>
              <p>
                * indicates equal contribution
              </p>
            </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!--/ Preprints -->


        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/VideoTemp_o3.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos</span>
              <br>
              Wenqi Liu*, <a href="https://scholar.google.com/citations?user=GUiPxp0AAAAJ&hl=en">Yunxiao Wang*</a>, <strong>Shijie Ma*</strong>, <a href="https://mengliu1991.github.io/">Meng Liu</a>, Qile Su, <a href="https://scholar.google.com/citations?user=nsKcpUEAAAAJ&hl=en">Tianke Zhang</a>, Haonan Fan, Changyi Liu, Kaiyu Jiang, Jiankang Chen, Kaiyu Tang, <a href="https://scholar.google.com/citations?user=Hmgqwf4AAAAJ&hl=en">Bin Wen</a>, Fan Yang, Tingting Gao, <a href="https://scholar.google.com/citations?user=IJGli9AAAAAJ&hl=en">Han Li</a>, <a href="https://weiyinwei.github.io/">Yinwei Wei</a>, <a href="https://xuemengsong.github.io/">Xuemeng Song</a>
              <br>
              <a href="https://arxiv.org/abs/2602.07801">Paper</a> / <a href="https://arxiv.org/abs/2602.07801">arXiv</a> / <a href="https://liuwq-bit.github.io/VideoTemp-o3/">Project Page</a> / <a href="https://github.com/Liuwq-bit/VideoTemp-o3">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/HappyPlus.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Happy++: Towards Stable and Unified Continual Generalized Category Discovery</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <a href="https://gyx-gloria.github.io/">Yuxin Guo</a>, <a href="https://moenupa.github.io/">Meng Wang</a>, <a href="https://scholar.google.com/citations?user=toVhUOgAAAAJ&hl=en">Wenzhuo Liu</a>, <a href="https://zhunzhong.site/">Zhun Zhong</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ&hl=en">Cheng-Lin Liu</a>
              <br>
              <a href="https://d197for5662m48.cloudfront.net/documents/publicationstatus/308311/preprint_pdf/7f6cc2a6efd850e43352b2f59073a401.pdf">Paper</a> / <a href="https://www.techrxiv.org/users/1026396/articles/1389660-happy-towards-stable-and-unified-continual-generalized-category-discovery">arXiv</a> / <a href="https://github.com/mashijie1028/Happy-CGCD">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Survey_CIL.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">A Comprehensive Survey on Continual Learning in Generative Models</span>
              <br>
              <a href="https://scholar.google.com/citations?user=p1eW_uwAAAAJ&hl=en">Haiyang Guo</a>, <a href="https://aurorazengfh.github.io/">Fanhu Zeng</a>, <a href="https://impression2805.github.io/">Fei Zhu</a>, Jiayi Wang, Xukai Wang, Jingang Zhou, <a href="https://bjzhb666.github.io/">Hongbo Zhao</a>, <a href="https://scholar.google.com/citations?user=toVhUOgAAAAJ&hl=en">Wenzhuo Liu</a>, <strong>Shijie Ma</strong>, <a href="https://scholar.google.com/citations?user=s8UL7aUAAAAJ&hl=en">Da-Han Wang</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>, <a href="https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=en">Cheng-Lin Liu</a>
              <br>
              <a href="https://arxiv.org/abs/2506.13045">arXiv</a> / <a href="https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Survey_OWL.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Open-world Machine Learning: A Review and New Outlooks</span>
              <br>
              <a href="https://impression2805.github.io/">Fei Zhu*</a>, <strong>Shijie Ma*</strong>, <a href="https://scholar.google.com/citations?user=zcwWjhUAAAAJ&hl=en">Zhen Cheng</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>, <a href="https://scholar.google.com/citations?user=qxWfV6cAAAAJ&hl=en">Zhaoxiang Zhang</a>, <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en">Dacheng Tao</a>, <a href="https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=en">Cheng-Lin Liu</a>
              <br>
              <a href="https://arxiv.org/abs/2403.01759">arXiv</a>
              <br>
            </td>
          </tr>


          <!-- Publications -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Publications</h2>
            </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!--/ Publications -->


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CVPR26_AudioStory.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">AudioStory: Generating Long-Form Narrative Audio with Large Language Models</span>
              <br>
              <a href="https://gyx-gloria.github.io/">Yuxin Guo</a>, <a href="http://ttengwang.com/">Teng Wang</a>, <a href="https://geyuying.github.io/">Yuying Ge</a>, <strong>Shijie Ma</strong>, <a href="https://geyixiao.com/">Yixiao Ge</a>, <a href="https://people.ucas.ac.cn/~zouwei">Wei Zou</a>, <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2026
              <br>
              <a href="https://arxiv.org/pdf/2508.20088">Paper</a> / <a href="https://arxiv.org/pdf/2508.20088">arXiv</a> / <a href="https://github.com/TencentARC/AudioStory">Project Page</a> / <a href="https://github.com/TencentARC/AudioStory">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICLR26_AdaPatch.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">One Patch Doesn't Fit All: Adaptive Patching for Native-Resolution Multimodal Large Language Models</span>
              <br>
              <a href="https://scholar.google.com/citations?user=toVhUOgAAAAJ&hl=en">Wenzhuo Liu</a>, <a href="https://scholar.google.com/citations?hl=zh-CN&user=-tceNNgAAAAJ&hl=en">Weijie Yin</a>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <strong>Shijie Ma</strong>, <a href="https://scholar.google.com/citations?user=p1eW_uwAAAAJ&hl=en">Haiyang Guo</a>, <a href="https://scholar.google.com/citations?user=XoiT9wMAAAAJ&hl=en">Yi Chen</a>, Xiao-Hui Li, Xiao Liang, Chao Feng, <a href="https://scholar.google.com/citations?user=8r3y8IMAAAAJ&hl=en">Cheng-Lin Liu</a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2026
              <br>
              <a href="https://openreview.net/forum?id=six75YUGgS">Paper</a> / <a href="https://openreview.net/forum?id=six75YUGgS">arXiv</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICCV25_GenHancer.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://geyuying.github.io/">Yuying Ge</a>, <a href="http://ttengwang.com/">Teng Wang</a>, <a href="https://gyx-gloria.github.io/">Yuxin Guo</a>, <a href="https://geyixiao.com/">Yixiao Ge</a>, <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>
              <br>
              <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ma_GenHancer_Imperfect_Generative_Models_are_Secretly_Strong_Vision-Centric_Enhancers_ICCV_2025_paper.html">Paper</a> / <a href="https://arxiv.org/abs/2503.19480">arXiv</a> / <a href="https://mashijie1028.github.io/GenHancer/">Project Page</a> / <a href="https://github.com/mashijie1028/GenHancer">Code</a> / <a href="https://huggingface.co/msj9817/GenHancer">Model</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TPAMI25_ProtoGCD.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2025
              <br>
              <a href="https://ieeexplore.ieee.org/document/10948388">Paper</a> / <a href="https://arxiv.org/abs/2504.03755">arXiv</a> / <a href="https://github.com/mashijie1028/ProtoGCD">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICLR25_Dolphin.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Aligned Better, Listen Better for Audio-Visual Large Language Models</span>
              <br>
              <a href="https://gyx-gloria.github.io/">Yuxin Guo</a>, <a href="https://scholar.google.com/citations?user=dNhzCu4AAAAJ&hl=en">Shuailei Ma</a>, <strong>Shijie Ma</strong>, <a href="https://scholar.google.com/citations?user=gSI_eiIAAAAJ&hl=en">Xiaoyi Bao</a>, <a href="https://scholar.google.com/citations?user=UHCDCRMAAAAJ&hl=en">Chen-Wei Xie</a>, <a href="https://zkcys001.github.io/">Kecheng Zheng</a>, Tingyu Weng, <a href="https://scholar.google.com/citations?user=IK9DwSIAAAAJ&hl=en">Siyang Sun</a>, <a href="https://scholar.google.com/citations?user=-hFpScAAAAAJ&hl=en">Yun Zheng</a>, <a href="https://people.ucas.ac.cn/~zouwei">Wei Zou</a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025
              <br>
              <a href="https://openreview.net/forum?id=1SYUKPeM12">Paper</a> / <a href="https://arxiv.org/abs/2504.02061">arXiv</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/PR25_TrustDD.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Towards Trustworthy Dataset Distillation</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <a href="https://scholar.google.com/citations?user=zcwWjhUAAAAJ&hl=en">Zhen Cheng</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>
              <br>
              <em>Pattern Recognition (<strong>PR</strong>)</em>, 2025
              <br>
              <a href="https://doi.org/10.1016/j.patcog.2024.110875">Paper</a> / <a href="https://arxiv.org/abs/2307.09165">arXiv</a> / <a href="https://github.com/mashijie1028/TrustDD">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/NeurIPS24_MSPE.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution</span>
              <br>
              <a href="https://scholar.google.com/citations?user=toVhUOgAAAAJ&hl=en">Wenzhuo Liu</a>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <strong>Shijie Ma</strong>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>
              <br>
              <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/3396657fe1a3c9a43ac7cd809c51a41e-Abstract-Conference.html">Paper</a> / <a href="https://arxiv.org/abs/2405.18240">arXiv</a> / <a href="https://github.com/SmallPigPeppa/MSPE">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/NeurIPS24_Happy.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Happy: A Debiased Learning Framework for Continual Generalized Category Discovery</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <a href="https://zhunzhong.site/">Zhun Zhong</a>, <a href="https://scholar.google.com/citations?user=toVhUOgAAAAJ&hl=en">Wenzhuo Liu</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>
              <br>
              <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/5ae0f7cfd65d8e2b39da4177fef82015-Abstract-Conference.html">Paper</a> / <a href="https://arxiv.org/abs/2410.06535">arXiv</a> / <a href="https://github.com/mashijie1028/Happy-CGCD">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ECCV24_WPSSAM.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation Models</span>
              <br>
              Xin-Jian Wu, <a href="https://evergrow.github.io/">Ruisong Zhang</a>, Jie Qin, <strong>Shijie Ma</strong>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024 (<em><strong style="color:red;">Oral</strong></em>)
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-72784-9_18">Paper</a> / <a href="https://arxiv.org/abs/2407.10131">arXiv</a> / <a href="https://github.com/xjwu1024/WPS-SAM">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CVPR24_CrossMAE.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training</span>
              <br>
              <a href="https://gyx-gloria.github.io/">Yuxin Guo</a>, <a href="https://scholar.google.com/citations?user=IK9DwSIAAAAJ&hl=en">Siyang Sun</a>, <a href="https://scholar.google.com/citations?user=dNhzCu4AAAAJ&hl=en">Shuailei Ma</a>, <a href="https://zkcys001.github.io/">Kecheng Zheng</a>, <a href="https://scholar.google.com/citations?user=gSI_eiIAAAAJ&hl=en">Xiaoyi Bao</a>, <strong>Shijie Ma</strong>, <a href="https://people.ucas.ac.cn/~zouwei">Wei Zou</a>, <a href="https://scholar.google.com/citations?user=-hFpScAAAAAJ&hl=en">Yun Zheng</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Guo_CrossMAE_Cross-Modality_Masked_Autoencoders_for_Region-Aware_Audio-Visual_Pre-Training_CVPR_2024_paper.html">Paper</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CVPR24_AGCD.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Active Generalized Category Discovery</span>
              <br>
              <strong>Shijie Ma</strong>, <a href="https://impression2805.github.io/">Fei Zhu</a>, <a href="https://zhunzhong.site/">Zhun Zhong</a>, <a href="https://scholar.google.com/citations?user=xl11SEMAAAAJ&hl=en">Xu-Yao Zhang</a>, <a href="https://scholar.google.com/citations?hl=en&user=8r3y8IMAAAAJ">Cheng-Lin Liu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ma_Active_Generalized_Category_Discovery_CVPR_2024_paper.html">Paper</a> / <a href="https://arxiv.org/abs/2403.04272">arXiv</a> / <a href="https://github.com/mashijie1028/ActiveGCD">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICASSP24_XPL.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization</span>
              <br>
              <a href="https://gyx-gloria.github.io/">Yuxin Guo</a>, <strong>Shijie Ma</strong>, Yuhao Zhao, <a href="https://people.ucas.ac.cn/~zouwei">Wei Zou</a>
              <br>
              <em>International Conference on Acoustics, Speech, and Signal Processing (<strong>ICASSP</strong>)</em>, 2024
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10447949">Paper</a> / <a href="https://arxiv.org/abs/2403.03095">arXiv</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/NeurIPS23_DMT.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization</span>
              <br>
              <a href="https://gyx-gloria.github.io/">Yuxin Guo</a>, <strong>Shijie Ma</strong>, <a href="https://people.ucas.edu.cn/~suhu">Hu Su</a>, Zhiqing Wang, Yuhao Zhao, <a href="https://people.ucas.ac.cn/~zouwei">Wei Zou</a>, <a href="https://scholar.google.com/citations?user=IK9DwSIAAAAJ&hl=en">Siyang Sun</a>, <a href="https://scholar.google.com/citations?user=-hFpScAAAAAJ&hl=en">Yun Zheng</a>
              <br>
              <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/98143953a7fd1319175b491888fc8df5-Abstract-Conference.html">Paper</a> / <a href="https://arxiv.org/abs/2403.03145">arXiv</a> / <a href="https://github.com/gyx-gloria/DMT">Code</a>
              <br>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TNN22_SNN.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <span class="papertitle">Rethinking Pretraining as a Bridge From ANNs to SNNs</span>
              <br>
              <a href="https://lyh983012.github.io/">Yihan Lin</a>, <a href="https://scholar.google.com/citations?user=NBsnGgoAAAAJ&hl=en">Yifan Hu</a>, <strong>Shijie Ma</strong>, <a href="https://manutdmoon.github.io/">Dongjie Yu</a>, <a href="https://scholar.google.com/citations?user=qCfE--MAAAAJ&hl=en">Guoqi Li</a>
              <br>
              <em>IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>)</em>, 2022
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9950361">Paper</a> / <a href="https://arxiv.org/abs/2203.01158">arXiv</a> / <a href="https://github.com/lyh983012/SNN-ANN-Pretrain">Code</a>
              <br>
            </td>
          </tr>





          <!-- Honors and Awards -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Honors and Awards</h2>
                <p>
                  <li style="margin: 5px;"> 
                    ICML Top Reviewer, 2025
                  </li>
                  <li style="margin: 5px;"> 
                    National Scholarship, 2024
                  </li>
                  <li style="margin: 5px;"> 
                    NeurIPS Top Reviewer, 2024
                  </li>
                  <li style="margin: 5px;"> 
                    Merit Student, Chinese Academy of Sciences, 2022
                  </li>
                  <li style="margin: 5px;"> 
                    Freshmen Scholarship, Chinese Academy of Sciences, 2021
                  </li>
                  <li style="margin: 5px;"> 
                    Comprehensive Scholarship, Tsinghua University, 2020
                  </li>
                  <li style="margin: 5px;"> 
                    Academic Scholarship, Tsinghua University, 2019
                  </li>
                </p>
              </td>
            </tr>
          </tbody></table>
          <!--/ Honors and Awards -->

          <!-- Academic Services -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Academic Services</h2>
                <p>
                  <li style="margin: 5px;"> 
                    <b>Conference Reviewer:</b>  NeurIPS, NeurIPS D&B Track, ICML, ICLR, CVPR, ICCV, ECCV, AAAI, AISTATS, WACV, BMVC
                  </li>
                  <li style="margin: 5px;"> 
                    <b>Journal Reviewer:</b> IJCV, IEEE TKDE, IEEE TCSVT, TMLR
                  </li>
                </p>
              </td>
            </tr>
          </tbody></table>
          <!--/ Academic Services -->

          <!-- template -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <a href="https://github.com/jonbarron/jonbarron_website">Website Template</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          <!--/ template -->
        </td>
      </tr>
    </table>

    <p><center>
      <div id="clustrmaps-widget" style="width:5%">
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=kDzMXgXjIOklJc-WKvp73xL1liE7Bx-rJP3q1EAjZdk"></script>
      </div>
      <br>
        &copy; Shijie Ma | Last updated: February, 2026
    </center></p>
  </body>
</html>
